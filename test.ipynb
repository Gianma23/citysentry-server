{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\citysentry-server\\citysentry\\Lib\\site-packages\\google\\cloud\\firestore_v1\\base_collection.py:303: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
      "  return query.where(field_path, op_string, value)\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_24016\\2962080406.py:24: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
      "  query = reports_ref.where(\"timestamp\", \">=\", start_date).where(\"timestamp\", \"<=\", end_date)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                tags  \\\n",
      "0                  [Litter, Unsafe Bridge, Graffiti]   \n",
      "1                                 [Cracked Pavement]   \n",
      "2                                      [Faded Paint]   \n",
      "3  [Overgrown Vegetation, Dead Animals, Illegal D...   \n",
      "4                                 [Cracked Pavement]   \n",
      "\n",
      "                                               photo  \\\n",
      "0  /9j/4AAQSkZJRgABAQAAAQABAAD/4gIoSUNDX1BST0ZJTE...   \n",
      "1  /9j/4AAQSkZJRgABAQAAAQABAAD/4gIoSUNDX1BST0ZJTE...   \n",
      "2  /9j/4AAQSkZJRgABAQAAAQABAAD/4QOcaHR0cDovL25zLm...   \n",
      "3  /9j/4AAQSkZJRgABAQAAAQABAAD/4QOcaHR0cDovL25zLm...   \n",
      "4  /9j/4AAQSkZJRgABAQAAAQABAAD/4gIoSUNDX1BST0ZJTE...   \n",
      "\n",
      "                                            location                 timestamp  \n",
      "0  {'latitude': 43.7209372, 'longitude': 10.3884053} 2025-01-28 13:24:06+00:00  \n",
      "1  {'latitude': 43.7223825, 'longitude': 10.3961468} 2025-01-28 15:26:50+00:00  \n",
      "2   {'latitude': 43.720375, 'longitude': 10.3882175} 2025-01-28 21:48:29+00:00  \n",
      "3  {'latitude': 43.7114451, 'longitude': 10.4066152} 2025-01-29 08:14:01+00:00  \n",
      "4  {'latitude': 43.7224215, 'longitude': 10.3969367} 2025-01-30 07:16:22+00:00  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_24016\\2962080406.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cluster'] = db.labels_\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_24016\\2962080406.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cluster'] = db.labels_\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_24016\\2962080406.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cluster'] = db.labels_\n",
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_24016\\2962080406.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['cluster'] = db.labels_\n",
      "e:\\citysentry-server\\citysentry\\Lib\\site-packages\\google\\cloud\\firestore_v1\\base_collection.py:303: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
      "  return query.where(field_path, op_string, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   volume        lon        lat          group                 start_date\n",
      "0       9  10.404716  43.731139  environmental  2025-01-27 10:01:35+00:00\n",
      "1       9  10.404716  43.731139  environmental  2025-01-26 10:01:35+00:00\n",
      "2       7  10.404349  43.730936  environmental  2025-01-25 10:01:35+00:00\n",
      "3       7  10.404349  43.730936  environmental  2025-01-24 10:01:35+00:00\n",
      "4       3  10.388128  43.722098  environmental  2025-01-23 10:01:35+00:00\n",
      "Predicted 2.0 clusters for group 'environmental'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\citysentry-server\\citysentry\\Lib\\site-packages\\google\\cloud\\firestore_v1\\base_collection.py:303: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
      "  return query.where(field_path, op_string, value)\n",
      "e:\\citysentry-server\\citysentry\\Lib\\site-packages\\google\\cloud\\firestore_v1\\base_collection.py:303: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
      "  return query.where(field_path, op_string, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   volume        lon        lat           group                 start_date\n",
      "0      11  10.398435  43.720089  infrastructure  2025-01-27 10:01:35+00:00\n",
      "1      12  10.397291  43.720139  infrastructure  2025-01-26 10:01:35+00:00\n",
      "2      12  10.397291  43.720139  infrastructure  2025-01-25 10:01:35+00:00\n",
      "3      15  10.397746  43.720440  infrastructure  2025-01-24 10:01:35+00:00\n",
      "4      12  10.399673  43.720482  infrastructure  2025-01-23 10:01:35+00:00\n",
      "Predicted 1.0 clusters for group 'infrastructure'.\n",
      "   volume        lon        lat   group                 start_date\n",
      "0       5  10.406527  43.715834  safety  2025-01-27 10:01:35+00:00\n",
      "1       4  10.389254  43.722345  safety  2025-01-27 10:01:35+00:00\n",
      "2       5  10.406527  43.715834  safety  2025-01-26 10:01:35+00:00\n",
      "3       4  10.389254  43.722345  safety  2025-01-26 10:01:35+00:00\n",
      "4       4  10.389254  43.722345  safety  2025-01-25 10:01:35+00:00\n",
      "Predicted 1.0 clusters for group 'safety'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\citysentry-server\\citysentry\\Lib\\site-packages\\google\\cloud\\firestore_v1\\base_collection.py:303: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
      "  return query.where(field_path, op_string, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   volume        lon        lat      group                 start_date\n",
      "0      13  10.406509  43.717397  aesthetic  2025-01-27 10:01:35+00:00\n",
      "1       3  10.389333  43.721849  aesthetic  2025-01-27 10:01:35+00:00\n",
      "2      16  10.407299  43.718539  aesthetic  2025-01-26 10:01:35+00:00\n",
      "3       3  10.389333  43.721849  aesthetic  2025-01-26 10:01:35+00:00\n",
      "4      15  10.407818  43.718467  aesthetic  2025-01-25 10:01:35+00:00\n",
      "Predicted 2.0 clusters for group 'aesthetic'.\n",
      "[43.720207 10.40674  16.004765]\n",
      "[43.72349   10.402606   5.8067846]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "Predictions saved to Firestore.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "import torch.nn as nn\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cred = credentials.Certificate(\"city-sentry-firebase.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "#############################################\n",
    "#               Cluster Data\n",
    "#############################################\n",
    "end_date = datetime.now(timezone.utc)\n",
    "start_date = end_date - timedelta(weeks=2)\n",
    "\n",
    "reports_ref = db.collection(\"reports\")\n",
    "query = reports_ref.where(\"timestamp\", \">=\", start_date).where(\"timestamp\", \"<=\", end_date)\n",
    "docs = query.stream()\n",
    "reports = [doc.to_dict() for doc in docs]\n",
    "df = pd.DataFrame(reports)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "tag_groups = {\n",
    "    'environmental': ['Litter', 'Illegal Dumping', 'Air Pollution', 'Water Pollution'],\n",
    "    'infrastructure': ['Pothole', 'Cracked Pavement', 'Broken Streetlight', 'Damaged Bench', 'Blocked Drainage', 'Abandoned Vehicle'],\n",
    "    'safety': ['Vandalism', 'Unsafe Building', 'Unsafe Bridge', 'Broken Traffic Signals', 'Open Manholes'],\n",
    "    'aesthetic': ['Overgrown Vegetation', 'Graffiti', 'Neglected Monuments', 'Faded Paint'],\n",
    "}\n",
    "\n",
    "eps = 0.008\n",
    "min_samples = 3\n",
    "\n",
    "def cluster_reports_by_group(data, group_name, tags):\n",
    "    results = []\n",
    "    data = data[data['tags'].apply(lambda x: any(tag in tags for tag in x))]\n",
    "    if not data.empty:\n",
    "        # Extract coordinates from the 'location' field\n",
    "        coords = np.array([[loc['latitude'], loc['longitude']] for loc in data['location']])\n",
    "\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples).fit(coords)\n",
    "        data['cluster'] = db.labels_\n",
    "        \n",
    "        # For each identified cluster (ignoring noise with label -1)\n",
    "        for cluster_id in set(db.labels_):\n",
    "            if cluster_id != -1:\n",
    "                cluster_points = data[data['cluster'] == cluster_id]\n",
    "                # Calculate the centroid as the mean latitude and longitude\n",
    "                centroid_lat = cluster_points['location'].apply(lambda loc: loc['latitude']).mean()\n",
    "                centroid_lon = cluster_points['location'].apply(lambda loc: loc['longitude']).mean()\n",
    "                \n",
    "                results.append({\n",
    "                    'group': group_name,\n",
    "                    'start_date': start_date,\n",
    "                    'lat': centroid_lat,\n",
    "                    'lon': centroid_lon,\n",
    "                    'volume': len(cluster_points),\n",
    "                })\n",
    "    \n",
    "    return results\n",
    "\n",
    "all_results = []\n",
    "for group_name, tags in tag_groups.items():\n",
    "    group_results = cluster_reports_by_group(df, group_name, tags)\n",
    "    all_results.extend(group_results)\n",
    "\n",
    "clusters_df = pd.DataFrame(all_results)\n",
    "\n",
    "clusters_data = clusters_df.to_dict(orient='records')\n",
    "collection = db.collection(\"clusters\")\n",
    "for record in clusters_data:\n",
    "    collection.add(record)\n",
    "    \n",
    "#############################################\n",
    "#            Model Definition\n",
    "#############################################\n",
    "\n",
    "class DeepSetEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64, output_dim=64):\n",
    "        \"\"\"\n",
    "        Encodes a set of clusters (each with `input_dim` features) into a fixed-length vector.\n",
    "        \"\"\"\n",
    "        super(DeepSetEncoder, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch, num_clusters, 3)\n",
    "        \"\"\"\n",
    "        mask = (x.abs().sum(dim=-1) != 0).float()  # Mask out zero-padded clusters\n",
    "        embed = self.mlp(x)\n",
    "\n",
    "        aggregated_avg = (embed * mask.unsqueeze(-1)).sum(dim=2) / mask.sum(dim=2).clamp(min=1).unsqueeze(-1)\n",
    "        count = mask.sum(dim=2).unsqueeze(-1)  # shape: (batch, seq_len, 1)\n",
    "        aggregated = torch.cat([aggregated_avg, count], dim=-1)\n",
    "\n",
    "        return aggregated  # Average the embeddings of all clusters\n",
    "\n",
    "class ForecastModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, lstm_hidden_dim, future_steps=1, max_clusters=10):\n",
    "        super(ForecastModel, self).__init__()\n",
    "        self.future_steps = future_steps\n",
    "        self.max_clusters = max_clusters\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.deepset = DeepSetEncoder(input_dim, hidden_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim+1, lstm_hidden_dim, num_layers=2, batch_first=True, dropout=0.1)\n",
    "        \n",
    "        self.cluster_count_predictor = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, lstm_hidden_dim//2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(lstm_hidden_dim//2, 1),\n",
    "        )\n",
    "        \n",
    "        self.cluster_decoder = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden_dim, lstm_hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(lstm_hidden_dim, max_clusters * input_dim)\n",
    "        )  # Predict cluster properties\n",
    "\n",
    "    def forward(self, x):\n",
    "        aggregated = self.deepset(x)\n",
    "        lstm_out, _ = self.lstm(aggregated)\n",
    "        # Predict the number of clusters\n",
    "        num_clusters = self.cluster_count_predictor(lstm_out[:, -1, :])\n",
    "\n",
    "        # Predict cluster properties for each cluster\n",
    "        cluster_predictions = self.cluster_decoder(lstm_out[:, -1, :])\n",
    "        cluster_predictions = cluster_predictions.view(-1, self.max_clusters, self.input_dim)\n",
    "        # Mask out predictions for non-existent clusters\n",
    "        mask = torch.arange(self.max_clusters, device=cluster_predictions.device).unsqueeze(0) < num_clusters.round()\n",
    "        cluster_predictions = cluster_predictions * mask.unsqueeze(-1).float()\n",
    "        cluster_predictions = torch.where(cluster_predictions == 0, torch.tensor(0.0, device=cluster_predictions.device), cluster_predictions)\n",
    "        \n",
    "        return torch.squeeze(num_clusters), torch.unsqueeze(cluster_predictions, 1)\n",
    "\n",
    "\n",
    "#############################################\n",
    "\n",
    "MAX_CLUSTERS = 10\n",
    "SEQ_LEN = 30\n",
    "PADDING_VALUE = 0.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for group in tag_groups.keys():\n",
    "    clusters_ref = db.collection(\"clusters\")\n",
    "    query = clusters_ref.where(\"group\", \"==\", group).order_by(\"start_date\", direction=firestore.Query.DESCENDING)\n",
    "    docs = query.stream()\n",
    "    clusters = [doc.to_dict() for doc in docs]\n",
    "    clusters_df = pd.DataFrame(clusters)\n",
    "    if clusters_df.empty:\n",
    "        print(f\"No clusters found for group '{group}'.\")\n",
    "        continue\n",
    "    print(clusters_df.head())\n",
    "    clusters_df['start_date'] = pd.to_datetime(clusters_df['start_date'])\n",
    "    unique_dates = clusters_df['start_date'].sort_values(ascending=False).unique()[:SEQ_LEN]\n",
    "\n",
    "    # Create a (SEQ_LEN, MAX_CLUSTERS, 3) tensor\n",
    "    tensor_data = np.full((SEQ_LEN, MAX_CLUSTERS, 3), PADDING_VALUE, dtype=np.float32)\n",
    "    \n",
    "    for i, date in enumerate(unique_dates):\n",
    "        date_clusters = clusters_df[clusters_df['start_date'] == date].head(MAX_CLUSTERS)  # Take up to MAX_CLUSTERS for that date\n",
    "        tensor_data[i, :len(date_clusters), :] = date_clusters[['lat', 'lon', 'volume']].to_numpy()\n",
    "    \n",
    "    input_tensor = torch.tensor(tensor_data).unsqueeze(0)\n",
    "    \n",
    "    # Load the model and perform inference\n",
    "    model = ForecastModel(input_dim=3, \n",
    "                        hidden_dim=128,\n",
    "                        lstm_hidden_dim=512,\n",
    "                        future_steps=1,\n",
    "                        max_clusters=MAX_CLUSTERS)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(f\"models/seq_30_eps_0.008_window_14_{group}_model.pth\"))  # Update with your model path\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():        \n",
    "        n_clusters, predictions = model(input_tensor.to(device))\n",
    "        print(f\"Predicted {n_clusters.round()} clusters for group '{group}'.\")\n",
    "        predicted_clusters = predictions.squeeze(0).squeeze(0).detach().cpu().numpy()  # shape: (max_clusters, features)\n",
    "\n",
    "    # Get a timestamp for record keeping\n",
    "    timestamp = (datetime.now(timezone.utc) + timedelta(days=7)).isoformat()\n",
    "\n",
    "# Iterate over each cluster and save it as a separate document\n",
    "for idx, cluster in enumerate(predicted_clusters):\n",
    "    print(cluster)\n",
    "    if cluster[2] <= 0:  \n",
    "        continue\n",
    "\n",
    "    cluster_data = {\n",
    "        \"group\": group,\n",
    "        \"pred_date\": timestamp,\n",
    "        \"latitude\": float(cluster[0]),\n",
    "        \"longitude\": float(cluster[1]),\n",
    "        \"volume\": float(cluster[2]),\n",
    "    }\n",
    "    \n",
    "    collection = db.collection(\"predictions\")\n",
    "    collection.add(cluster_data)\n",
    "\n",
    "print(\"Predictions saved to Firestore.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citysentry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
